\subsubsection{Definition: energiebasierte Modelle und energiebasiertes Lernen}

\glspl{ebm} finden im maschinellen Lernen Anwendung und arbeiten mithilfe einer Energiefunktion, welche jeder beliebigen Konfiguration an Variablen eine skalare Energie zuweist. Am Beispiel eines neuronalen Netzes könnten diese Variablen die Eingabe-Variablen, die Parameter- und Versteckten-Variablen sowie die Ausgabe-Variablen sein. Zur Inferenz des Modells werden zunächst die Eingabe-Variablen festgelegt und anschließend ein Minimum der Energiefunktion bestimmt. Wurde ein Minimum gefunden, kann die Ausgabe des Modells anhand der Ausgabe-Variablen ausgelesen werden. Ein \ac{ebm} wird durch Anpassung seiner Energiefunktion trainiert, indem sie kleinere Energien für korrekte Werte und größere Energien für falsche Werte generiert. \cite{Lecun2006} Die ersten vorgestellten \ac{ebm} waren das Hopfield-Netzwerk \cite{Hopfield1984} und die auf dem Hopfield-Netzwerk basierende Boltzmann-Maschine \cite{Ackley1985}.
