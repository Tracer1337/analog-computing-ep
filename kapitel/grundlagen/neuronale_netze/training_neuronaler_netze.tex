\subsubsection{Training neuronaler Netze mit Backpropagation und Gradient Descent}
\label{chap:Training neuronaler Netze mit Backpropagation und Gradient Descent}

Das Gradienten-Verfahren ist ein Optimierungs-Algorithmus, der mithilfe einer Fehlerfunktion die Parameter eines Modells so anpasst, dass die Fehlerfunktion minimiert wird. Um das zu erreichen, muss das Verfahren erst die Steigung der Fehlerfunktion berechnen können, um dann iterativ die Parameter anzupassen. (\cite[vgl. S. 118]{Geron2019})

\begin{quote}
  "`Neurons wire together if they fire together"' (\cite{Lowel1992})
\end{quote}

Dieses Zitat prägt das sog. Hebbian learning, eine Regel die beschreibt wie sich die Gewichte zwischen Neuronen relativ zu deren Aktivierungen verändern. Ein Perceptron wird mit einer Abwandlung dieser Regel trainiert, die außerdem den Fehler der Ausgabe mit einbezieht und diejenigen Gewichte verstärkt, die zu einer Verringerung des Fehlers führen. (\cite[vgl. S. 289 ff.]{Geron2019}) Die Lernregel lautet damit:

\textbf{Formel \ref{eq:Hebbian-Learning angewendet auf ein Perceptron}: Hebbian-Learning angewendet auf ein Perceptron}
\begin{flalign}
  {w_{i,j}}^{(next step)}=w_{i,j}+\eta(y_j-\hat{y}_j)
  \label{eq:Hebbian-Learning angewendet auf ein Perceptron}
\end{flalign}
\cite[Quelle: ][S. 289]{Geron2019}

Ein Verfahren zum Trainieren eines \ac{mlp} stellt das von \cite{Rumelhart1986} vorgestellte Backpropagation dar. Dieses basiert auf dem Gradienten-Verfahren, mit der Eigenschaft die Gradienten der Gewichte in allen Ebenen des \ac{mlp} mit Bezug auf jeden einzelnen Parameter effizient berechnen zu können. Die Trainingsdaten werden in mehreren Epochen im folgenden Ablauf durchlaufen:

Zuerst wird für jede Instanz der Trainingsdaten das \ac{mlp} im Forward Pass durchlaufen. Die Ausgabe jedes Neurons der versteckten Ebenen wird dabei zwischengespeichert. Nun wird die Ausgabe des \ac{mlp} anhand der Fehlerfunktion bestimmt. Die Gradienten aller Gewichte zwischen Neuronen der versteckten Ebenen werden im Reverse Pass bestimmt. Dazu wird der Beitrag jedes Ausgabe-Neuronen zum Fehler berechnet und gleiches rekursiv für die Neuronen der versteckten Ebenen wiederholt. Mit den berechneten Werten kann abschließend das Gradienten-Verfahren angewendet werden. (\cite[S. 286]{Geron2019})
