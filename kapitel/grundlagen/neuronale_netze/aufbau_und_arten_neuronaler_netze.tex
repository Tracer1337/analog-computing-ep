\subsubsection{Aufbau und Arten neuronaler Netze}

\begin{quote}
  "`Birds inspired us to fly, burdock plants inspired Velcro, and nature has in- spired countless more inventions. It seems only logical then, to look at the brain’s architecture for inspiration on how to build an intelligent machine"' (\cite[S. 279]{Geron2019})
\end{quote}

Neuronale Netze wurden erstmals 1943 von den Wissenschaftlern Warren S. McCulloch und Walter Pitts in ihrer gemeinsamen Arbeit „A logical calculus of the ideas immanent in nervous activity“ \cite{McCulloch1943} eingeführt. Sie stellten ein vereinfachtes Modell eines künstlichen Neurons vor, das lediglich aus binären Eingaben und einer binären Ausgabe besteht und seine Ausgabe aktiviert, sobald sich eine bestimmte Anzahl an Eingabewerten aktiviert. McCulloch und Pitts zeigten, dass dieser einfache Baustein ausreicht, um jeden möglichen logischen Ausdruck als neuronales Netz darzustellen.

Neuronale Netze zeichnen sich mittlerweile durch ihre Vielseitigkeit, Leistungsfähigkeit und Skalierbarkeit aus und haben damit maßgeblich zur Gründung eines neuen Forschungsfeldes, dem „Deep Learning“, beigetragen. \cite{Geron2019} Die neu gewonnen Aufmerksamkeit im Zusammenhang mit Deep Learning brachte Innovationen wie das von OpenAI entwickelte Sprachmodell „GPT-4“ oder der Bildgenerierungs-KI „Midjourney“ hervor, wodurch bewiesen wurde, dass neuronale Netze zur Lösung komplexer Aufgaben geeignet sind und sogar teilweise ähnlich brauchbare Ergebnisse wie ein Mensch liefern können. \cite{OpenAI2024}

Eines der einfachsten neuronalen Netze ist das von \cite{Rosenblatt1958} vorgestellte \gls{perceptron}, dieses basiert auf dem Konzept der \gls{tlu}. Die Eingabe- und Ausgabewerte einer \gls{tlu} sind numerisch und den eingehenden Verbindungen ist jeweils eine Gewichtung zugewiesen. Die \gls{tlu} berechnet nun die gewichtete Summe der Eingabewerte \(z=w_1x_1+w_2x_2 ... +w_nx_n=xtw\). Unter Anwendung einer Aktivierungsfunktion \(hw(x)=step(z)\) kann nun berechnet werden, ob die \gls{tlu} den Wert 0 oder 1 ausgibt. Eine Aktivierungsfunktion für diese Art der Neuronen ist i. d. R. eine Heaviside-Funktion oder eine Vorzeichen-Funktion. \cite[vgl. S. 284 ff.]{Geron2019}

Aktivierungsfunktionen aus Buch einfügen

Abbildung aus Buch nachstellen

Eine alleinstehende \gls{tlu} kann ausschließlich für lineare binäre Klassifikation genutzt werden, es berechnet eine gewichtete Summe anhand der Eingabewerte und gibt einen positiven oder negativen Ausgabewert, abhängig von der Überschreitung eines Schwellenwertes. Ein \gls{perceptron} stellt nun eine Sammlung dieser Einheiten auf einer einzelnen Ebene dar, wobei jede \gls{tlu} mit jeder Eingabe verbunden ist. Dies wird als \gls{dense_layer} oder \gls{fully_connected_layer} bezeichnet. Die Eingabewerte des \gls{perceptron} werden durch Eingabe-Neuronen geschleust, wozu zusätzlich ein \gls{bias_neuron} gezählt wird. Dieses Neuron gibt konstant den Wert 1 aus und dient dazu, jedem Neuron des \gls{perceptron} eine Bias-Gewichtung zuzuweisen. Die Ausgabe eines \gls{perceptron} berechnet sich durch \(h_W,b(X)=\rho(XW+b)\) \cite[vgl. S. 284 ff.]{Geron2019}

Das \gls{perceptron} kann nun in mehreren Ebenen genutzt werden, um ein \gls{mlp} zu erzeugen. Dieses besteht aus einer Eingabe-Ebene, mindestens einer versteckten Ebene und einer Ausgabe-Ebene. Jede dieser Ebenen ist im \gls{mlp} mit der jeweils nächsten Ebene vollständig verbunden. Das \gls{mlp} kann durch die vorwärts gerichteten Verbindungen auch als \gls{fnn} bezeichnet werden, eines mit vielen versteckten Ebenen wird \gls{dnn} genannt. \cite[vgl. S. 284 ff.]{Geron2019}

Das bisher beschriebene \gls{mlp} kann zur Klassifikation genutzt werden. Um dieses auch auf Regressionen anwenden zu können, muss die Aktivierungsfunktion entweder entfernt oder durch z.B. ReLU ausgetauscht werden, damit die Ausgabeneuronen willkürliche Werte annehmen können. Die Anzahl der Ausgabe-Neuronen muss in dem Fall angepasst werden, sodass jeder geforderte Ausgabewert durch ein Neuron abgebildet ist. \cite[vgl. S. 292 ff.]{Geron2019}

Eine weitere Art des neuronalen Netz ist das \gls{cnn}, dessen Hauptbestandteil die Convolutional-Ebenen sind. Diese Ebenen sind nur jeweils mit einem Ausschnitt der vorherigen Ebene verbunden, wodurch das daraus entstehende neuronale Netz abstrakte Eigenschaften der Eingabe-Neuronen erlernen kann. Da die einzelnen Ebenen hier nicht, wie beim \gls{mlp}, vollständig verbunden sind, erlaubt ein \gls{cnn} eine große Anzahl an Neuronen pro Ebene, ohne den Rechenaufwand für Training und Inferenz exponentiell zu steigern. Durch diese Eigenschaften eignet sich das \gls{cnn} besonders für die Bildbearbeitung, da hier als Eingabe die Pixel genutzt und darin Eigenschaften des Bildes gefunden werden können. \cite[vgl. S. 447 f.]{Geron2019}

Das \gls{rnn} ähnelt vom Aufbau dem \gls{fnn} unterscheidet sich aber durch rückwärts gerichtete Verbindungen. Die Ausgabe eines Neuron wird damit Teil seiner Eingabe, womit es sich Informationen "`merken"' kann. Ein \gls{rnn} kann als Sequenz-zu-Sequenz Netzwerk genutzt werden, es erhält also eine Sequenz an Eingabe-Werten und produziert eine Sequenz an Ausgabe-Werten. Genauso kann jeder Ausgabewert bis auf den letzten ignoriert werden, was als Sequenz-zu-Vektor Netzwerk bezeichnet wird. Andersherum kann ein \gls{rnn} auch einen Vektor als Eingabe erhalten und eine Sequenz ausgeben, wodurch z.B. eine Beschreibung für ein Bild generiert werden könnte. Letztlich ist auch ein sog. Encoder-Decoder Modell möglich, wobei ein Sequenz-zu-Vektor Netzwerk zuerst eine Repräsentation der Eingabesequenz erzeugt, und ein Vektor-zu-Sequenz Netzwerk daraufhin eine Ausgabe aus dieser Repräsentation erzeugt. Ein Anwendungsfall dafür sind Sprachanwendungen wie ein Übersetzer, da jedes Wort eines Textes im Vornherein bekannt sein muss, um eine korrekte Ausgabe zu erzeugen. \cite[vgl. S. 497 ff.]{Geron2019}
