\subsubsection{Aufbau und Arten neuronaler Netze}

\begin{quote}
"`Birds inspired us to fly, burdock plants inspired Velcro, and nature has in- spired countless more inventions. It seems only logical then, to look at the brain’s architecture for inspiration on how to build an intelligent machine"' (\cite[S. 279]{Geron2019})
\end{quote}

Neuronale Netze wurden erstmals 1943 von den Wissenschaftlern Warren S. McCulloch und Walter Pitts in ihrer gemeinsamen Arbeit „A logical calculus of the ideas immanent in nervous activity“ (\cite{McCulloch1943}) eingeführt. Sie stellten ein vereinfachtes Modell eines künstlichen Neurons vor, das lediglich aus binären Eingaben und einer binären Ausgabe besteht und seine Ausgabe aktiviert, sobald sich eine bestimmte Anzahl an Eingabewerten aktiviert. McCulloch und Pitts zeigten, dass dieser einfache Baustein ausreicht, um jeden möglichen logischen Ausdruck als neuronales Netz darzustellen.

Neuronale Netze zeichnen sich mittlerweile durch ihre Vielseitigkeit, Leistungsfähigkeit und Skalierbarkeit aus und haben damit maßgeblich zur Gründung eines neuen Forschungsfeldes, dem „Deep Learning“, beigetragen. (\cite{Geron2019}) Die neu gewonnen Aufmerksamkeit im Zusammenhang mit Deep Learning brachte Innovationen wie das von OpenAI entwickelte Sprachmodell „GPT-4“ oder der Bildgenerierungs-KI „Midjourney“ hervor, wodurch bewiesen wurde, dass neuronale Netze zur Lösung komplexer Aufgaben geeignet sind und sogar teilweise ähnlich brauchbare Ergebnisse wie ein Mensch liefern können. (\cite{OpenAI2024})

Eines der einfachsten neuronalen Netze ist das von \cite{Rosenblatt1958} vorgestellte Perceptron, dieses basiert auf dem Konzept der \ac{TLU}. Die Eingabe- und Ausgabewerte einer \ac{TLU} sind numerisch und den eingehenden Verbindungen ist jeweils eine Gewichtung zugewiesen. Die \ac{TLU} berechnet nun die gewichtete Summe der Eingabewerte \(z=w_1x_1+w_2_x_2 ... +w_nx_n=xtw\). Unter Anwendung einer Aktivierungsfunktion \(hw(x)=step(z)\) kann nun berechnet werden, ob die \ac{TLU} den Wert 0 oder 1 ausgibt. Eine Aktivierungsfunktion für diese Art der Neuronen ist i. d. R. eine Heaviside-Funktion oder eine Vorzeichen-Funktion. \cite[S. 284 ff.]{Geron2019}

Abbildung aus Buch nachstellen

Eine alleinstehende \ac{TLU} kann ausschließlich für lineare binäre Klassifikation genutzt werden. Ein Perceptron stellt nun eine Sammlung dieser Einheiten auf einer einzelnen Ebene dar, wobei jede \ac{TLU} mit jeder Eingabe verbunden ist. Dies wird als \gls{dense_layer} oder \gls{fully_connected_layer} bezeichnet. 