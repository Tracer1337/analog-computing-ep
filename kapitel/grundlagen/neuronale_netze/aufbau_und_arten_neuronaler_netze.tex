\subsubsection{Aufbau und Arten neuronaler Netze}

\begin{quote}
  "`Birds inspired us to fly, burdock plants inspired Velcro, and nature has in- spired countless more inventions. It seems only logical then, to look at the brain’s architecture for inspiration on how to build an intelligent machine"' (\cite[S. 279]{Geron2019})
\end{quote}

Die Wissenschaftler Warren S. McCulloch und Walter Pitts führten neuronale Netze erstmals 1943 in ihrer gemeinsamen Arbeit „A logical calculus of the ideas immanent in nervous activity“ (\cite{McCulloch1943}) ein. Sie entwickelten ein vereinfachtes Modell eines künstlichen Neurons, das ausschließlich binäre Eingaben und eine binäre Ausgabe besitzt. Die Aktivierung der Ausgabe erfolgt, sobald eine festgelegte Anzahl an Eingabewerten aktiviert ist. McCulloch und Pitts konnten zeigen, dass bereits dieses einfache Modell ausreicht, um beliebige logische Ausdrücke in Form eines neuronalen Netzes darzustellen.

Heute sind neuronale Netze für ihre Vielseitigkeit, Leistungsfähigkeit und Skalierbarkeit bekannt, was wesentlich zur Entstehung des Forschungsfeldes „Deep Learning“ beigetragen hat. (\cite{Geron2019}) Die mit Deep Learning verbundene Aufmerksamkeit führte zu innovativen Entwicklungen wie dem Sprachmodell „GPT-4“ von OpenAI oder der Bildgenerierungs-KI „Midjourney“. Diese Fortschritte zeigen, dass neuronale Netze in der Lage sind komplexe Probleme zu lösen und in manchen Fällen sogar Ergebnisse zu liefern, die mit denen eines Menschen vergleichbar sind. (\cite{OpenAI2024})

Eines der einfachsten neuronalen Netze ist das von \cite{Rosenblatt1958} vorgestellte Perceptron, dieses basiert auf dem Konzept der \ac{tlu}. Die Eingabe- und Ausgabewerte einer \ac{tlu} sind numerisch und den eingehenden Verbindungen ist jeweils ein Gewicht zugewiesen. Die \ac{tlu} berechnet nun die gewichtete Summe der Eingabewerte \(z=w_1x_1+w_2x_2 ... +w_nx_n=xtw\). Unter Anwendung einer Aktivierungsfunktion \(hw(x)=step(z)\) kann nun berechnet werden, ob die \ac{tlu} den Wert 0 oder 1 ausgibt. Eine Aktivierungsfunktion für diese Art der Neuronen ist i. d. R. eine Heaviside-Funktion oder eine Vorzeichen-Funktion. (\cite[vgl. S. 284 ff.]{Geron2019})

Eine alleinstehende \ac{tlu} kann ausschließlich für lineare binäre Klassifikation genutzt werden, es berechnet eine gewichtete Summe anhand der Eingabewerte und gibt einen positiven oder negativen Ausgabewert, abhängig von der Überschreitung eines Schwellenwertes. Ein Perceptron stellt nun eine Sammlung dieser Einheiten auf einer einzelnen Ebene dar, wobei jede \ac{tlu} mit jeder Eingabe verbunden ist. Dies wird als Dense Layer oder Fully Connected Layer bezeichnet. Die Eingabewerte des Perceptron werden durch Eingabe-Neuronen geschleust, wozu zusätzlich ein Bias-Neuron gezählt wird. Dieses Neuron gibt konstant den Wert 1 aus und dient dazu, jedem Neuron des Perceptron einen Bias-Wert zuzuweisen. Die Ausgabe eines Perceptron berechnet sich durch \(h_W,b(X)=\rho(XW+b)\) (\cite[vgl. S. 284 ff.]{Geron2019})

Das Perceptron kann nun in mehreren Ebenen genutzt werden, um ein \ac{mlp} zu erzeugen. Dieses besteht aus einer Eingabe-Ebene, mindestens einer versteckten Ebene und einer Ausgabe-Ebene. Jede dieser Ebenen ist im \ac{mlp} mit der jeweils nächsten Ebene vollständig verbunden. Das \ac{mlp} kann durch die vorwärts gerichteten Verbindungen auch als \ac{fnn} bezeichnet werden, eines mit vielen versteckten Ebenen wird \ac{dnn} genannt. (\cite[vgl. S. 284 ff.]{Geron2019})

Das bisher beschriebene \ac{mlp} kann zur Klassifikation genutzt werden. Um dieses auch auf Regressionen anwenden zu können, muss die Aktivierungsfunktion entweder entfernt oder durch z.B. ReLU ausgetauscht werden, damit die Ausgabeneuronen willkürliche Werte annehmen können. Die Anzahl der Ausgabe-Neuronen muss in dem Fall angepasst werden, sodass jeder geforderte Ausgabewert durch ein Neuron abgebildet ist. (\cite[vgl. S. 292 ff.]{Geron2019})

Eine weitere Art des neuronalen Netz ist das \ac{cnn}, dessen Hauptbestandteil die Convolutional-Ebenen sind. Diese Ebenen sind nur jeweils mit einem Ausschnitt der vorherigen Ebene verbunden, wodurch das daraus entstehende neuronale Netz abstrakte Eigenschaften der Eingabe-Neuronen erlernen kann. Da die einzelnen Ebenen hier nicht, wie beim \ac{mlp}, vollständig verbunden sind, erlaubt ein \ac{cnn} eine große Anzahl an Neuronen pro Ebene, ohne den Rechenaufwand für Training und Inferenz exponentiell zu steigern. Durch diese Eigenschaften eignet sich das \ac{cnn} besonders für die Bildbearbeitung, da hier als Eingabe die Pixel genutzt und darin Eigenschaften des Bildes gefunden werden können. (\cite[vgl. S. 447 f.]{Geron2019})

Das \ac{rnn} ähnelt vom Aufbau dem \ac{fnn} unterscheidet sich aber durch rückwärts gerichtete Verbindungen. Die Ausgabe eines Neuron wird damit Teil seiner Eingabe, womit es sich Informationen "`merken"' kann. Ein \ac{rnn} kann als Sequenz-zu-Sequenz Netzwerk genutzt werden, es erhält also eine Sequenz an Eingabe-Werten und produziert eine Sequenz an Ausgabe-Werten. Genauso kann jeder Ausgabewert bis auf den letzten ignoriert werden, was als Sequenz-zu-Vektor Netzwerk bezeichnet wird. Andersherum kann ein \ac{rnn} auch einen Vektor als Eingabe erhalten und eine Sequenz ausgeben, wodurch z.B. eine Beschreibung für ein Bild generiert werden könnte. Letztlich ist auch ein sog. Encoder-Decoder Modell möglich, wobei ein Sequenz-zu-Vektor Netzwerk zuerst eine Repräsentation der Eingabesequenz erzeugt, und ein Vektor-zu-Sequenz Netzwerk daraufhin eine Ausgabe aus dieser Repräsentation erzeugt. Ein Anwendungsfall dafür sind Sprachanwendungen wie ein Übersetzer, da jedes Wort eines Textes im Vornherein bekannt sein muss, um eine korrekte Ausgabe zu erzeugen. (\cite[vgl. S. 497 ff.]{Geron2019})
