\subsection{Potenziale des Equilibrium Propagation in analogen Systemen}
\label{chap:Potenziale des Equilibrium Propagation in analogen Systemen}

Einige Arbeiten befassen sich bereits mit dem Trainieren neuronaler Netze auf analoger Hardware. So finden Algorithmen wie das "`Stochastic Hamiltonian Descent"' (\cite{Onen2022}) oder "`Analog Gradient Accumulation with Dynamic Reference"' (\cite{Rasch2024}) Anwendung auf asymmetrische Netzwerke, implementiert auf resistiven Crossbar-Arrays (ähnlich \zb dem lucidac, siehe Kapitel \ref{chap:Typische Komponenten und Bauweisen analoger Computer}), benötigen aber Informationen über das gesamte Netzwerk und vorheriger Zustände dessen. Nennenswert sind auch das "`Multiplexed Gradient Descent"' (\cite{McCaughan2023}) sowie das "`Physical Neuronal Network"' (\cite{Sakemi2024}), denen aber die biologische Plausibilität des \ac{ep} fehlt.

Als Alternative zum \ac{bptt} und Annäherung an die Funktionsweise biologischer Neuronen findet das \ac{c-ep} Anwendung an energiebasierten Modellen (\cite{Ernoult2020}). Diese Arbeit befasst sich in Bezug auf das \ac{bptt} mit einem unpassenden Netzwerk, da sich das \ac{hnn} durch eine einzige Ebene an Neuronen auszeichnet und dadurch die räumliche Lokalität des \ac{c-ep} nicht vollkommen ausnutzen kann. Das \ac{c-ep} hat Potenzial für die Anwendung auf ein mehrlagiges \ac{hnn} oder \ac{mlp}, da hier die Schwierigkeit in der Anwendung des Fehlers des Netzwerks über mehrere Ebenen liegt (vgl. Kapitel \ref{chap:Training neuronaler Netze mit Backpropagation und Gradient Descent}). Wie \cite{Martin2020} auch zeigte, eignet sich \ac{ep} als Basis für das Trainieren von \ac{snn} und ist damit eine vielversprechende Alternative zu herkömmlichen Gradienten-basierten Methoden, insbesondere für energieeffiziente und hardwarefreundliche Implementierungen in neuromorphen Systemen.
